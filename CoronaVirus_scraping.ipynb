{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'D:\\Corona\\Corona_Analysis')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.thehindu.com/sport/cricket/coronavirus-former-cricketer-sukhvinder-singh-recounts-his-rescue-mission-to-italy/article31167345.ece\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.thehindu.com/topic/coronavirus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url)\n",
    "soup=bs(html.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page link\n",
    "page_link=soup.select(\"[class='pagination']\")\n",
    "page_link[0].find_all('a')[0]['href']\n",
    "\n",
    "# last page number on this topic\n",
    "last_link=page_link[0].find_all('a')[-1]['href']\n",
    "last_page=re.search(\"page=\\d*\",last_link).group(0)\n",
    "last_page_number=int(last_page.replace(\"page=\",\"\"))\n",
    "last_page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterarte over all pages in the range to create and get the link\n",
    "upd_url=[]\n",
    "title=[]\n",
    "link=[]\n",
    "page_number=[]\n",
    "for i in range(1,last_page_number+1):\n",
    "    add_page_number=url+\"?page=\"+str(i)\n",
    "    #upd_url.append(add_page_number)\n",
    "    page_url=add_page_number\n",
    "    print(page_url)\n",
    "    html_page = requests.get(page_url)\n",
    "    soup_page=bs(html_page.content,\"html.parser\")\n",
    "    all_h3=soup_page.find_all(\"h3\")\n",
    "\n",
    "    for j in all_h3:\n",
    "        try:\n",
    "            title.append(j.find('a').get_text().replace(\"\\n\",\"\"))\n",
    "            link.append(j.find('a')['href'])\n",
    "            page_number.append(i)\n",
    "        except AttributeError:\n",
    "            print(\"skip\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'page_number':page_number,'title':title,\"link\":link})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('link.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_value = requests.get(link_value)\n",
    "# link_soup=bs(html_value.content,\"html.parser\")\n",
    "# article = link_soup.select(\"[class~=article]\")\n",
    "# at_c=article[0].select(\"[class*='author-container hidden-xs']\")[0]\n",
    "# at_c.find_all('span')[0].get_text().replace(\"\\n\",\"\")\n",
    "# print(at_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_list = []\n",
    "title_list = []\n",
    "content_list = []\n",
    "author_list = []\n",
    "creation_date_list = [] \n",
    "counter = 1140\n",
    "link_pinged=[]\n",
    "for link_value in link[1140:]:\n",
    "    try:\n",
    "        \n",
    "        print(counter)\n",
    "        try:\n",
    "            html_value = requests.get(link_value)\n",
    "        except ValueError:\n",
    "            html_value = None\n",
    "        link_soup=bs(html_value.content,\"html.parser\")\n",
    "        article = link_soup.select(\"[class~=article]\")\n",
    "        sec=article[0].select(\"[class~=section-name]\")\n",
    "        try:\n",
    "            section=sec[0].get_text().replace(\"\\n\",\"\")\n",
    "        except IndexError:\n",
    "            section = None\n",
    "        title = article[0].select(\"[class~=title]\")\n",
    "        try:\n",
    "            title = title[0].get_text().replace(\"\\n\",\"\")\n",
    "        except IndexError:\n",
    "            title = None\n",
    "        body = article[0].select(\"[id*='content-body-']\")\n",
    "        try:\n",
    "            content=body[0].get_text().replace(\"\\n\",\"\")\n",
    "        except IndexError:\n",
    "            content = None\n",
    "\n",
    "        ut_c = article[0].select(\"[class*='ut-container']\")[0]\n",
    "        #author_c=article[0].select(\"[class*='author-container*']\")[0]\n",
    "        at_c=article[0].select(\"[class*='author-container hidden-xs']\")[0]\n",
    "        try:\n",
    "            author_name = at_c.find_all('span')[0].get_text().replace(\"\\n\",\"\")\n",
    "        except IndexError:\n",
    "            author_name = None\n",
    "        #print(author_name)\n",
    "        #author_name = ut_c.find_all('span')[0].get_text().replace(\"\\n\",\"\")\n",
    "        #print(author_name)\n",
    "        try:\n",
    "            creation_date = ut_c.find_all('span')[1].get_text().replace(\"\\n\",\"\")\n",
    "        except IndexError:\n",
    "            creation_date = None\n",
    "        #print(creation_date)\n",
    "        # update the list \n",
    "        section_list.append(section)\n",
    "        title_list.append(title)\n",
    "        content_list.append(content)\n",
    "        author_list.append(author_name)\n",
    "        creation_date_list.append(creation_date)\n",
    "        link_pinged.append(link_value)\n",
    "        counter = counter + 1\n",
    "    except ChunkedEncodingError:\n",
    "        print(link_value)\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collected = {'Author':author_list,'Date':creation_date_list,'Title':title_list,'Section':section_list, 'content': content_list, 'url':link_pinged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.DataFrame(data_collected)\n",
    "\n",
    "final_df.to_excel(\"The_hindu_extract_corona_remaining.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(author_list))\n",
    "print(len(creation_date_list))\n",
    "print(len(title_list))\n",
    "print(len(section_list))\n",
    "print(len(content_list))\n",
    "print(len(link[:660]))\n",
    "\n",
    "title_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Author':author_list,'Date':creation_date_list,'Title':title_list,'Section':section_list, 'content': content_list, 'url':link}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
